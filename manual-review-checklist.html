<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Labor Relations Platform - Manual Review Checklist</title>
<style>
  @media print {
    body { font-size: 11pt; }
    .page-break { page-break-before: always; }
    h1 { page-break-after: avoid; }
    h2 { page-break-after: avoid; }
    table { page-break-inside: avoid; }
    .no-print { display: none; }
  }
  body {
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    max-width: 8.5in;
    margin: 0.5in auto;
    padding: 0 0.25in;
    color: #1a1a1a;
    line-height: 1.5;
  }
  h1 { font-size: 22pt; border-bottom: 3px solid #1a1a1a; padding-bottom: 8px; margin-top: 0; }
  h2 { font-size: 16pt; color: #2c3e50; border-bottom: 1px solid #bdc3c7; padding-bottom: 4px; margin-top: 28px; }
  h3 { font-size: 13pt; color: #34495e; margin-top: 20px; }
  table { border-collapse: collapse; width: 100%; margin: 12px 0; font-size: 10pt; }
  th, td { border: 1px solid #bdc3c7; padding: 6px 8px; text-align: left; }
  th { background: #2c3e50; color: white; font-weight: 600; }
  tr:nth-child(even) { background: #f8f9fa; }
  .checkbox { width: 14px; height: 14px; border: 2px solid #555; display: inline-block; margin-right: 6px; vertical-align: middle; }
  .item { margin: 8px 0; padding-left: 4px; }
  .risk-high { background: #fde8e8; }
  .risk-med { background: #fef3cd; }
  .risk-low { background: #d4edda; }
  .tag { display: inline-block; padding: 1px 8px; border-radius: 3px; font-size: 9pt; font-weight: bold; color: white; }
  .tag-critical { background: #c0392b; }
  .tag-high { background: #e67e22; }
  .tag-medium { background: #f39c12; }
  .tag-low { background: #27ae60; }
  .tag-done { background: #95a5a6; }
  .note { background: #eaf2f8; border-left: 4px solid #2980b9; padding: 8px 12px; margin: 10px 0; font-size: 10pt; }
  .warning { background: #fdf2e9; border-left: 4px solid #e67e22; padding: 8px 12px; margin: 10px 0; font-size: 10pt; }
  .section-number { color: #7f8c8d; font-size: 10pt; }
  code { background: #ecf0f1; padding: 1px 4px; border-radius: 2px; font-size: 9.5pt; }
  .toc a { text-decoration: none; color: #2c3e50; }
  .toc li { margin: 4px 0; }
  .generated { font-size: 9pt; color: #95a5a6; text-align: right; }
  .sub-item { margin-left: 24px; }
</style>
</head>
<body>

<h1>Labor Relations Research Platform<br>Manual Review Checklist</h1>
<p class="generated">Generated: February 12, 2026 | Database: olms_multiyear (33 GB, 157 tables)</p>

<div class="note">
<strong>How to use this document:</strong> Print to PDF (Ctrl+P) for offline review. Each checkbox item represents something that needs human judgment -- automated tests can't catch these. Write notes in the margins or on a separate sheet.
</div>

<h2>Table of Contents</h2>
<ol class="toc">
  <li><a href="#s1">Match Quality Spot-Checks (50-100 records each)</a></li>
  <li><a href="#s2">Scoring Weight Validation</a></li>
  <li><a href="#s3">Scorecard Tier Thresholds</a></li>
  <li><a href="#s4">NLRB Prediction Model Weights</a></li>
  <li><a href="#s5">Gower Similarity Feature Weights</a></li>
  <li><a href="#s6">Reference Data Currency</a></li>
  <li><a href="#s7">Known Deferred Bugs</a></li>
  <li><a href="#s8">Data Gap Inventory</a></li>
  <li><a href="#s9">Technical Debt (Open Items)</a></li>
  <li><a href="#s10">SAM.gov Matching Status</a></li>
  <li><a href="#s11">Pre-Deployment Verification</a></li>
</ol>

<!-- ============================================================ -->
<div class="page-break"></div>
<h2 id="s1"><span class="section-number">1.</span> Match Quality Spot-Checks</h2>

<p>These fuzzy-matched records are the most likely to contain false positives. For each tier, pull a random sample and verify the match is correct.</p>

<h3>1.1 OSHA Tier D: State + NAICS Fuzzy (threshold 0.55)</h3>
<table>
  <tr><th>Detail</th><th>Value</th></tr>
  <tr><td>Script</td><td><code>scripts/etl/osha_match_phase5.py</code></td></tr>
  <tr><td>Table</td><td><code>osha_f7_matches</code> WHERE match_tier = 'D'</td></tr>
  <tr><td>Constraints</td><td>Same state + same NAICS 2-digit prefix only (NO city/zip/address)</td></tr>
  <tr><td>Known risk</td><td>At 0.40 this produced 155K bad matches (USPS matched UPS). Raised to 0.55, but still loosest tier.</td></tr>
  <tr><td>Sample size needed</td><td>50-100 records, focusing on similarity 0.55-0.65 range</td></tr>
</table>
<p class="item"><span class="checkbox"></span> Pull sample query: <code>SELECT * FROM osha_f7_matches WHERE match_tier='D' AND match_confidence BETWEEN 0.55 AND 0.65 ORDER BY random() LIMIT 50</code></p>
<p class="item"><span class="checkbox"></span> For each: does the OSHA establishment name plausibly refer to the same company as the F7 employer?</p>
<p class="item"><span class="checkbox"></span> Estimate false positive rate. Acceptable: &lt;5%. If &gt;10%, raise threshold to 0.60.</p>
<p class="item"><span class="checkbox"></span> Check for common-word collisions: "American", "National", "Midwest", "Pacific", generic industry terms</p>

<h3>1.2 WHD Tier 6: Fuzzy Name + State (threshold 0.55)</h3>
<table>
  <tr><th>Detail</th><th>Value</th></tr>
  <tr><td>Script</td><td><code>scripts/etl/whd_match_phase5.py</code></td></tr>
  <tr><td>Table</td><td><code>whd_f7_matches</code> WHERE match_tier = 6</td></tr>
  <tr><td>Constraints</td><td>Same state only (no NAICS, no address)</td></tr>
  <tr><td>Known risk</td><td>Legal suffixes stripped aggressively -- "Restaurant Association" becomes "restaurant". Cross-industry collisions possible.</td></tr>
  <tr><td>Extra concern</td><td>Code only WARNS on duplicate case_ids, doesn't block insert</td></tr>
</table>
<p class="item"><span class="checkbox"></span> Pull 50 Tier 6 matches with confidence 0.55-0.65</p>
<p class="item"><span class="checkbox"></span> Check for different-industry collisions (e.g., "Smith Electric" the contractor vs "Smith Electronics" the retailer)</p>
<p class="item"><span class="checkbox"></span> Check for any duplicate case_ids mapped to different F7 employers</p>

<h3>1.3 IRS 990 Tier 4: Fuzzy Name + State (threshold 0.60)</h3>
<table>
  <tr><th>Detail</th><th>Value</th></tr>
  <tr><td>Script</td><td><code>scripts/etl/match_990_national.py</code></td></tr>
  <tr><td>Table</td><td><code>national_990_f7_matches</code> WHERE match_tier = 4</td></tr>
  <tr><td>Known risk</td><td>Nonprofits have generic names ("Community Center", "Foundation"). Two different nonprofits in same state easily collide at 0.60.</td></tr>
</table>
<p class="item"><span class="checkbox"></span> Pull 30-50 Tier 4 matches</p>
<p class="item"><span class="checkbox"></span> Verify nonprofits actually match the correct union-related employer (not a different nonprofit with similar name)</p>
<p class="item"><span class="checkbox"></span> Check EIN-based matches (Tiers 1-2) -- are there any cases where EIN LPAD normalization created false matches?</p>

<h3>1.4 SAM.gov Matching (in progress)</h3>
<table>
  <tr><th>Detail</th><th>Value</th></tr>
  <tr><td>Script</td><td><code>scripts/scoring/match_sam_to_employers.py</code></td></tr>
  <tr><td>Table</td><td><code>sam_f7_matches</code></td></tr>
  <tr><td>Critical bug</td><td>Tier A (exact name+state) got 0 matches -- SAM's <code>name_aggressive</code> normalization differs from F7's <code>employer_name_aggressive</code>. Need to investigate.</td></tr>
  <tr><td>Status</td><td>Tier B was in progress at last session end (201 matches at checkpoint). Tiers C/D not yet run.</td></tr>
</table>
<p class="item"><span class="checkbox"></span> Check if SAM matching completed or stalled</p>
<p class="item"><span class="checkbox"></span> Investigate Tier A name normalization mismatch: compare 10 SAM <code>name_aggressive</code> values to their F7 counterparts</p>
<p class="item"><span class="checkbox"></span> Verify cleanco was applied to SAM entities (it doubled GLEIF matches)</p>

<h3>1.5 Splink Probabilistic Matches</h3>
<table>
  <tr><th>Detail</th><th>Value</th></tr>
  <tr><td>Config</td><td><code>scripts/matching/splink_config.py</code></td></tr>
  <tr><td>Known risk</td><td>Level 3 (JW >= 0.80) allows city-name-prefix false positives: "Cleveland Cliffs" matched "Cleveland Ballet"</td></tr>
  <tr><td>Safe level</td><td>Level 4 (JW >= 0.88) for NEW_MATCH. Level 3 OK only when cross-confirmed by pg_trgm.</td></tr>
</table>
<p class="item"><span class="checkbox"></span> Review any NEW_MATCH entries in crosswalk that used name_level = 3 without pg_trgm confirmation</p>

<h3>1.6 F7 Employer Merges</h3>
<table>
  <tr><th>Detail</th><th>Value</th></tr>
  <tr><td>Script</td><td><code>scripts/cleanup/merge_f7_enhanced.py</code></td></tr>
  <tr><td>Merges done</td><td>1,210 (967 SPLINK_CONFIRMED + 243 NEW_MATCH)</td></tr>
  <tr><td>Concern</td><td>Keeper = largest + most-unionized employer, NOT highest match confidence. A large false positive could absorb the correct smaller record.</td></tr>
  <tr><td>Concern</td><td>City matching: 4-char substrings like "LA" would match "ATLANTA"</td></tr>
  <tr><td>Concern</td><td>COALESCE on crosswalk fields silently drops conflicting EINs/LEIs when keeper already has a value</td></tr>
</table>
<p class="item"><span class="checkbox"></span> Check for any merges where deleted record had higher match_confidence than keeper</p>
<p class="item"><span class="checkbox"></span> Check for crosswalk rows where COALESCE dropped a non-NULL EIN from a deleted record</p>

<!-- ============================================================ -->
<div class="page-break"></div>
<h2 id="s2"><span class="section-number">2.</span> Scorecard Weight Validation</h2>

<p>The 9-factor organizing scorecard drives all target recommendations. These weights were hand-picked, not derived from outcome data. Each needs human judgment.</p>

<table>
  <tr>
    <th>Factor</th><th>Max Pts</th><th>% of Total</th><th>Logic Summary</th><th>Review Question</th>
  </tr>
  <tr class="risk-high">
    <td><strong>Company Unions</strong></td><td>20</td><td>22%</td>
    <td>Binary: employer matched in osha_f7_matches = 20, else = 0</td>
    <td>Is a binary 0-or-20 too coarse? Should partial credit exist?</td>
  </tr>
  <tr>
    <td><strong>Industry Density</strong></td><td>10</td><td>11%</td>
    <td>>20%=10, >10%=8, >5%=5, &le;5%=2</td>
    <td>Are the 20%/10%/5% breakpoints meaningful for organizing?</td>
  </tr>
  <tr class="risk-med">
    <td><strong>Geographic</strong></td><td>10</td><td>11%</td>
    <td>NLRB win rate (0-4) + state density (0-3) + non-RTW (0-3)</td>
    <td>Is RTW status (3 pts) as important as NLRB win rate (4 pts)?</td>
  </tr>
  <tr class="risk-high">
    <td><strong>Size</strong></td><td>10</td><td>11%</td>
    <td>50-250=10 (sweet spot), 250-500=8, 25-50=6, 500-1000=4, else=2</td>
    <td><strong>CONFLICT:</strong> NLRB data shows 1-10 emp = 73.8% win rate (highest), but scorer gives small units only 2 pts. Is 50-250 the right sweet spot?</td>
  </tr>
  <tr>
    <td><strong>OSHA</strong></td><td>10</td><td>11%</td>
    <td>Ratio to industry avg: &ge;3x=7, &ge;2x=5, &ge;1.5x=4. Severity bonus up to +3</td>
    <td>Does 3x industry average reliably indicate organizing opportunity?</td>
  </tr>
  <tr>
    <td><strong>NLRB Prediction</strong></td><td>10</td><td>11%</td>
    <td>Predicted win%: &ge;82%=10, &ge;78%=8, &ge;74%=5, &ge;70%=3</td>
    <td>Range is narrow (69.5-85.3%). Does this compress scores too much?</td>
  </tr>
  <tr>
    <td><strong>Federal Contracts</strong></td><td>10</td><td>11%</td>
    <td>>$5M=10, >$1M=7, >$100K=4, >$0=2</td>
    <td>Are dollar thresholds meaningful? Should count of contracts matter too?</td>
  </tr>
  <tr>
    <td><strong>BLS Projections</strong></td><td>10</td><td>11%</td>
    <td>Growth >10%=10, >5%=7, >0%=4, &le;0%=2</td>
    <td>Is projected industry growth a real organizing signal?</td>
  </tr>
  <tr>
    <td><strong>Similarity</strong></td><td>10</td><td>11%</td>
    <td>Gower distance &ge;0.80=10, &ge;0.60=7, &ge;0.40=4</td>
    <td>Mean similarity is 0.92. Does this compress most employers to 10 pts?</td>
  </tr>
</table>

<div class="warning">
<strong>Key concern:</strong> "Company Unions" is worth 20 points (22% of total), more than double any other factor. An employer with ANY union match gets a 20-point head start. Is this intentional, or should the weight be reduced?
</div>

<p class="item"><span class="checkbox"></span> Review the 20-point binary "Company Unions" weight -- too dominant?</p>
<p class="item"><span class="checkbox"></span> Resolve the size scoring conflict: NLRB data favors 1-10 employees, but scorer favors 50-250</p>
<p class="item"><span class="checkbox"></span> Check if similarity score is compressed (mean 0.92 = almost everyone gets 10 pts)</p>
<p class="item"><span class="checkbox"></span> Decide: should BLS projected growth really count? Is it an organizing signal or noise?</p>
<p class="item"><span class="checkbox"></span> Pull 10 TOP-tier and 10 LOW-tier employers. Do the rankings feel right to an organizer?</p>

<!-- ============================================================ -->
<div class="page-break"></div>
<h2 id="s3"><span class="section-number">3.</span> Scorecard Tier Thresholds</h2>

<table>
  <tr><th>Tier</th><th>Score Range</th><th>Meaning</th></tr>
  <tr class="risk-high"><td><strong>TOP</strong></td><td>&ge; 30 / 90</td><td>High-priority organizing targets</td></tr>
  <tr class="risk-med"><td><strong>HIGH</strong></td><td>25 - 29</td><td>Secondary targets</td></tr>
  <tr><td><strong>MEDIUM</strong></td><td>20 - 24</td><td>Monitoring tier</td></tr>
  <tr><td><strong>LOW</strong></td><td>&lt; 20</td><td>Low priority</td></tr>
</table>

<div class="note">
Max possible score is 90, but practical max is ~70-80 (no employer will max every factor). Previous MEDIUM threshold was 15, raised to 20 during Phase 2 recalibration.
</div>

<p class="item"><span class="checkbox"></span> What % of employers fall in each tier? Is the distribution reasonable? (query: <code>SELECT CASE WHEN organizing_score >= 30 THEN 'TOP' WHEN organizing_score >= 25 THEN 'HIGH' WHEN organizing_score >= 20 THEN 'MEDIUM' ELSE 'LOW' END as tier, COUNT(*) FROM mergent_employers GROUP BY 1</code>)</p>
<p class="item"><span class="checkbox"></span> Should TOP be higher (e.g., 35) to be more selective?</p>
<p class="item"><span class="checkbox"></span> Is the gap between tiers (5 points each) meaningful or arbitrary?</p>

<!-- ============================================================ -->
<h2 id="s4"><span class="section-number">4.</span> NLRB Prediction Model Weights</h2>

<p>The <code>nlrb_predicted_win_pct</code> on <code>mergent_employers</code> uses a fixed-weight composite:</p>

<table>
  <tr><th>Component</th><th>Weight</th><th>Example Values</th><th>Question</th></tr>
  <tr>
    <td>State win rate</td><td>35%</td><td>CA: 78.2%, TX: 65.1%</td>
    <td>Are state effects really equal to industry effects?</td>
  </tr>
  <tr>
    <td>Industry win rate (NAICS 2-digit)</td><td>35%</td>
    <td>Healthcare(62): 87%, Mining(21): 61%</td>
    <td>26-point spread. Is 35% weight enough to capture this?</td>
  </tr>
  <tr>
    <td>Unit size bucket</td><td>20%</td>
    <td>1-10: 73.8%, 26-250: 63-65%</td>
    <td>Non-linear relationship. Should this be higher?</td>
  </tr>
  <tr class="risk-med">
    <td>Temporal trend</td><td>10%</td>
    <td>Flat +5.0% bonus for all</td>
    <td><strong>Is the 2022+ organizing wave permanent? Or a temporary bump?</strong></td>
  </tr>
</table>

<p><strong>Formula:</strong> <code>predicted = (state * 0.35) + (industry * 0.35) + (size * 0.20) + (trend * 0.10)</code></p>
<p><strong>Output range:</strong> 69.5% to 85.3% (narrow). National fallback: 67.4%.</p>

<p class="item"><span class="checkbox"></span> Is +5% temporal trend bonus justified going forward?</p>
<p class="item"><span class="checkbox"></span> Should weights be derived from logistic regression on actual outcomes instead of hand-picked?</p>
<p class="item"><span class="checkbox"></span> The narrow range (69.5-85.3%) means most employers cluster together. Does this differentiate enough?</p>

<!-- ============================================================ -->
<h2 id="s5"><span class="section-number">5.</span> Gower Similarity Feature Weights</h2>

<table>
  <tr><th>Feature</th><th>Weight</th><th>Type</th><th>Review Question</th></tr>
  <tr class="risk-med"><td>NAICS 4-digit</td><td><strong>3.0</strong></td><td>Hierarchical</td><td>Highest weight. Is industry match really 3x more important than location?</td></tr>
  <tr><td>Employees (site) log</td><td>2.0</td><td>Numeric</td><td>Double-weighted vs total employees. Correct?</td></tr>
  <tr><td>Employees (total) log</td><td>1.0</td><td>Numeric</td><td>--</td></tr>
  <tr><td>State</td><td>1.0</td><td>Categorical</td><td>Same weight as federal contractor status. Right?</td></tr>
  <tr><td>County</td><td>0.5</td><td>Categorical</td><td>Half of state. Reasonable?</td></tr>
  <tr><td>Company type</td><td>0.5</td><td>Categorical</td><td>--</td></tr>
  <tr><td>Is subsidiary</td><td>1.0</td><td>Binary</td><td>--</td></tr>
  <tr><td>Revenue log</td><td>1.0</td><td>Numeric</td><td>--</td></tr>
  <tr><td>Company age</td><td>0.5</td><td>Numeric</td><td>--</td></tr>
  <tr><td>OSHA violation rate</td><td>1.0</td><td>Numeric</td><td>Should violations be weighted higher for organizing?</td></tr>
  <tr><td>WHD violation rate</td><td>1.0</td><td>Numeric</td><td>--</td></tr>
  <tr><td>Federal contractor</td><td>1.0</td><td>Binary</td><td>--</td></tr>
  <tr><td>BLS growth %</td><td>0.5</td><td>Numeric</td><td>--</td></tr>
</table>

<p><strong>Total weight sum:</strong> 14.0. Industry alone is 21% of total distance calculation.</p>

<p class="item"><span class="checkbox"></span> Pull 5 employer-comparable pairs. Do they make intuitive sense? (hospital matches hospital, not auto dealer?)</p>
<p class="item"><span class="checkbox"></span> Should OSHA/WHD violation weights be higher since they indicate worker grievances?</p>
<p class="item"><span class="checkbox"></span> No ablation study exists. Consider: if you remove industry weight, do rankings change meaningfully?</p>

<!-- ============================================================ -->
<div class="page-break"></div>
<h2 id="s6"><span class="section-number">6.</span> Reference Data Currency</h2>

<p>These reference tables drive scoring. If they're stale, all scores are wrong.</p>

<table>
  <tr><th>Table</th><th>Rows</th><th>As Of</th><th>Review</th></tr>
  <tr>
    <td><code>ref_rtw_states</code></td><td>27</td><td>2025</td>
    <td><span class="checkbox"></span> Any states changed RTW status since 2025? (MI repealed 2023 -- is it still in the list?)</td>
  </tr>
  <tr>
    <td><code>ref_nlrb_state_win_rates</code></td><td>54</td><td>2020+ elections</td>
    <td><span class="checkbox"></span> Is the data cutoff recent enough? Should 2025-2026 elections be included?</td>
  </tr>
  <tr>
    <td><code>ref_nlrb_industry_win_rates</code></td><td>24</td><td>2020+ elections</td>
    <td><span class="checkbox"></span> Some industries have &lt;30 elections (low sample). Flag any with fewer than 10.</td>
  </tr>
  <tr>
    <td><code>ref_nlrb_size_win_rates</code></td><td>8 buckets</td><td>2020+ elections</td>
    <td><span class="checkbox"></span> Verify bucket boundaries align with real organizing patterns.</td>
  </tr>
  <tr>
    <td><code>ref_osha_industry_averages</code></td><td>340</td><td>Loaded data</td>
    <td><span class="checkbox"></span> National average: 2.23 violations/establishment. Is this current?</td>
  </tr>
  <tr>
    <td><code>bls_industry_projections</code></td><td>423</td><td>2024 BLS release</td>
    <td><span class="checkbox"></span> BLS updates projections annually. Is a refresh needed?</td>
  </tr>
</table>

<div class="warning">
<strong>Michigan RTW repeal:</strong> Michigan repealed its right-to-work law in March 2024. Verify that <code>ref_rtw_states</code> has been updated to remove MI. If not, Michigan employers are incorrectly penalized 3 points in geographic scoring.
</div>

<!-- ============================================================ -->
<h2 id="s7"><span class="section-number">7.</span> Known Deferred Bugs</h2>

<table>
  <tr><th>Bug</th><th>Impact</th><th>Severity</th></tr>
  <tr class="risk-high">
    <td><strong>Territory map is empty</strong> -- scorecard results lack lat/lon coordinates. Map renders but shows zero markers.</td>
    <td>Territory mode's primary visualization is broken</td>
    <td><span class="tag tag-high">HIGH</span></td>
  </tr>
  <tr class="risk-med">
    <td><strong>Race conditions on rapid mode switching</strong> -- switching Territory/Search/DeepDive while data loads can leave modals/charts in bad state.</td>
    <td>Edge case UI corruption</td>
    <td><span class="tag tag-medium">MEDIUM</span></td>
  </tr>
  <tr class="risk-med">
    <td><strong>Metros API ignores state parameter</strong> -- <code>/api/lookups/metros</code> returns all metros regardless of state filter.</td>
    <td>Metro dropdown shows irrelevant options</td>
    <td><span class="tag tag-medium">MEDIUM</span></td>
  </tr>
  <tr class="risk-med">
    <td><strong>Mergent data is NY-only</strong> -- 54K/56K unmatched Mergent employers are in New York. National coverage is nearly zero.</td>
    <td>Comparables and similarity scores are heavily NY-biased</td>
    <td><span class="tag tag-high">HIGH</span></td>
  </tr>
</table>

<p class="item"><span class="checkbox"></span> Decide: is the territory map critical enough to fix before any demo?</p>
<p class="item"><span class="checkbox"></span> Decide: is Mergent national expansion (manual CUNY library work, 3-5 hrs) worth doing now?</p>

<!-- ============================================================ -->
<div class="page-break"></div>
<h2 id="s8"><span class="section-number">8.</span> Data Gap Inventory</h2>

<h3>Current Match Rates</h3>
<table>
  <tr><th>Source</th><th>Total Records</th><th>Matched</th><th>Rate</th><th>Ceiling</th></tr>
  <tr><td>OSHA</td><td>1,007,217</td><td>138,340</td><td>13.7%</td><td>~15% (F7 universe limit)</td></tr>
  <tr><td>WHD</td><td>363,365</td><td>24,610</td><td>6.8%</td><td>~8-10%</td></tr>
  <tr><td>IRS 990</td><td>586,767</td><td>14,059</td><td>2.4%</td><td>~5%</td></tr>
  <tr><td>Mergent</td><td>56,431</td><td>~3,400</td><td>6.0%</td><td>&gt;15% (needs national data)</td></tr>
  <tr><td>SAM.gov</td><td>826,042</td><td>~201+</td><td>&lt;0.1%</td><td>Unknown (matching in progress)</td></tr>
</table>

<div class="note">
<strong>F7 ceiling:</strong> The F7 universe is ~61K union-related employers. Most OSHA/WHD employers aren't unionized. Match rates above 15% would require external employer data (SAM.gov, SEC full index, IRS BMF).
</div>

<h3>Missing Data Sources (not yet ingested)</h3>
<table>
  <tr><th>Source</th><th>Priority</th><th>What It Adds</th><th>Manual Action Needed</th></tr>
  <tr class="risk-high">
    <td>FMCS Contract Expirations</td>
    <td><span class="tag tag-critical">CRITICAL</span></td>
    <td>Contract expiration dates = #1 timing signal</td>
    <td><span class="checkbox"></span> Submit FOIA request to FMCS or check data.gov</td>
  </tr>
  <tr class="risk-high">
    <td>Mergent National</td>
    <td><span class="tag tag-high">HIGH</span></td>
    <td>150-200K employers nationally (vs 56K NY-only)</td>
    <td><span class="checkbox"></span> Extract from CUNY Mergent Intellect (3-5 hrs manual)</td>
  </tr>
  <tr class="risk-med">
    <td>SEC EDGAR Full Index</td>
    <td><span class="tag tag-medium">MED</span></td>
    <td>300K+ filers with EIN+CIK for crosswalk</td>
    <td>Automated download</td>
  </tr>
  <tr class="risk-med">
    <td>IRS BMF (nonprofits)</td>
    <td><span class="tag tag-medium">MED</span></td>
    <td>All nonprofits with EIN for matching</td>
    <td>Automated download</td>
  </tr>
  <tr>
    <td>SEC 10-K Exhibit 21</td>
    <td><span class="tag tag-low">LOW</span></td>
    <td>Parent/subsidiary mapping</td>
    <td>Automated but complex parsing</td>
  </tr>
  <tr class="risk-med">
    <td>FEC Political Contributions</td>
    <td><span class="tag tag-medium">MED</span></td>
    <td>PAC spending, anti-union lobbying data</td>
    <td>Automated from fec.gov</td>
  </tr>
  <tr>
    <td>State PERB Data (NY, CA, IL, NJ, OH)</td>
    <td><span class="tag tag-medium">MED</span></td>
    <td>Public sector bargaining unit data</td>
    <td><span class="checkbox"></span> Different process per state; some need public records requests</td>
  </tr>
  <tr>
    <td>MSHA / OFCCP / EBSA</td>
    <td><span class="tag tag-low">LOW</span></td>
    <td>Mine safety, contractor compliance, pension violations</td>
    <td><span class="checkbox"></span> MSHA free; OFCCP/EBSA may need FOIA</td>
  </tr>
</table>

<!-- ============================================================ -->
<div class="page-break"></div>
<h2 id="s9"><span class="section-number">9.</span> Technical Debt (Open Items)</h2>

<table>
  <tr><th>ID</th><th>Severity</th><th>Issue</th><th>Status</th></tr>
  <tr class="risk-high"><td>TD-03</td><td><span class="tag tag-critical">CRIT</span></td><td>Zero authentication on 142 API endpoints</td><td>PENDING (Phase 7.3)</td></tr>
  <tr class="risk-high"><td>TD-06</td><td><span class="tag tag-high">HIGH</span></td><td>10,500-line monolith frontend (single HTML file)</td><td>PENDING (Phase 8.1)</td></tr>
  <tr class="risk-high"><td>TD-08</td><td><span class="tag tag-high">HIGH</span></td><td>11+ endpoints return unbounded results (no pagination)</td><td>PENDING (Phase 7.2)</td></tr>
  <tr class="risk-med"><td>TD-10</td><td><span class="tag tag-high">HIGH</span></td><td>Project lives in Downloads folder</td><td>Move before deploy</td></tr>
  <tr class="risk-med"><td>TD-17</td><td><span class="tag tag-medium">MED</span></td><td>Scorecard weights unvalidated against outcomes</td><td>DEFERRED (Phase 10)</td></tr>
  <tr><td>TD-18</td><td><span class="tag tag-medium">MED</span></td><td>ETL scripts hardcoded to Windows paths</td><td>PENDING (Phase 9.6)</td></tr>
  <tr><td>TD-19</td><td><span class="tag tag-medium">MED</span></td><td>440+ scripts with no orchestration</td><td>PENDING (Phase 9.6)</td></tr>
  <tr><td>TD-20</td><td><span class="tag tag-medium">MED</span></td><td>Silent exception swallowing in some scripts</td><td>PENDING (Phase 7.4)</td></tr>
  <tr><td>TD-23</td><td><span class="tag tag-low">LOW</span></td><td>No CI/CD pipeline</td><td>PENDING (Phase 9.3)</td></tr>
  <tr><td>TD-24</td><td><span class="tag tag-low">LOW</span></td><td>Python 3.14-specific issues (target 3.12 for Docker)</td><td>PENDING</td></tr>
</table>

<p class="item"><span class="checkbox"></span> Decide: Is auth required before any external demo?</p>
<p class="item"><span class="checkbox"></span> Decide: Should project be moved from Downloads before more work?</p>

<!-- ============================================================ -->
<h2 id="s10"><span class="section-number">10.</span> SAM.gov Matching Status</h2>

<div class="warning">
<strong>Last known state (Feb 12, 2026):</strong> SAM ETL loaded 826,042 entities. Tier A (exact name) got 0 matches due to normalization mismatch. Tier B was running (201 matches at checkpoint). Tiers C and D have not started.
</div>

<p class="item"><span class="checkbox"></span> Check if matching is still running: <code>py scripts/etl/check_sam_progress.py</code></p>
<p class="item"><span class="checkbox"></span> If stalled, investigate Tier A name normalization: compare <code>SELECT name_aggressive FROM sam_entities LIMIT 10</code> vs <code>SELECT employer_name_aggressive FROM f7_employers_deduped LIMIT 10</code></p>
<p class="item"><span class="checkbox"></span> Key question: was <code>cleanco</code> applied to SAM names? It doubled GLEIF matches.</p>
<p class="item"><span class="checkbox"></span> Every SAM entity IS a federal contractor -- once matched, crosswalk gets UEI + CAGE + 6-digit NAICS for free</p>

<!-- ============================================================ -->
<div class="page-break"></div>
<h2 id="s11"><span class="section-number">11.</span> Pre-Deployment Verification</h2>

<h3>11.1 Functional Testing</h3>
<p class="item"><span class="checkbox"></span> Load territory dashboard with real data -- do all 7 sections render?</p>
<p class="item"><span class="checkbox"></span> Click through: Search --> employer detail --> back navigation</p>
<p class="item"><span class="checkbox"></span> Export territory report as PDF -- does formatting look right?</p>
<p class="item"><span class="checkbox"></span> Export targets CSV -- verify column names and row count</p>
<p class="item"><span class="checkbox"></span> Test deep dive mode for 3 different employer types (large/small, NY/non-NY, high/low score)</p>

<h3>11.2 Data Sanity</h3>
<p class="item"><span class="checkbox"></span> Run test suite: <code>py -m pytest tests/ -v</code> (expect 47/47 pass)</p>
<p class="item"><span class="checkbox"></span> Refresh materialized views: <code>REFRESH MATERIALIZED VIEW mv_employer_features; REFRESH MATERIALIZED VIEW mv_whd_employer_agg; REFRESH MATERIALIZED VIEW mv_employer_search;</code></p>
<p class="item"><span class="checkbox"></span> Verify crosswalk row count: <code>SELECT COUNT(*) FROM corporate_identifier_crosswalk</code> (expect ~25K+)</p>
<p class="item"><span class="checkbox"></span> Check for crosswalk orphans: <code>SELECT COUNT(*) FROM corporate_identifier_crosswalk c LEFT JOIN f7_employers_deduped f ON c.f7_employer_id = f.employer_id WHERE f.employer_id IS NULL</code></p>

<h3>11.3 API Smoke Test</h3>
<p class="item"><span class="checkbox"></span> <code>GET /api/health</code> -- returns 200</p>
<p class="item"><span class="checkbox"></span> <code>GET /api/organizing/scorecard?state=NY&limit=10</code> -- returns results with score breakdowns</p>
<p class="item"><span class="checkbox"></span> <code>GET /api/employers/search?name=walmart</code> -- returns employer records</p>
<p class="item"><span class="checkbox"></span> <code>GET /api/nlrb/patterns</code> -- returns industry/size/state win rates</p>
<p class="item"><span class="checkbox"></span> <code>GET /api/whd/top-violators?state=NY&limit=5</code> -- returns ranked violators</p>

<h3>11.4 Decisions Needed From You</h3>
<p class="item"><span class="checkbox"></span> <strong>Auth before demo?</strong> Currently zero auth on all 142 endpoints.</p>
<p class="item"><span class="checkbox"></span> <strong>Domain name?</strong> Needed for deployment (Phase 9.4).</p>
<p class="item"><span class="checkbox"></span> <strong>Cloud provider?</strong> Render (~$27/mo) vs DigitalOcean (~$84/mo).</p>
<p class="item"><span class="checkbox"></span> <strong>Beta testers?</strong> Identify 3-5 union organizers/researchers for soft launch.</p>
<p class="item"><span class="checkbox"></span> <strong>FMCS FOIA?</strong> Contract expiration data is the highest-impact missing source.</p>
<p class="item"><span class="checkbox"></span> <strong>Mergent expansion?</strong> 3-5 hours of manual CUNY library work to go from NY-only to national.</p>

<!-- ============================================================ -->
<div class="page-break"></div>
<h2>Quick Reference: Key Queries for Review</h2>

<p>Run these against the <code>olms_multiyear</code> database to pull data for the spot-checks above:</p>

<h3>Tier distribution</h3>
<pre>SELECT
  CASE WHEN organizing_score >= 30 THEN 'TOP'
       WHEN organizing_score >= 25 THEN 'HIGH'
       WHEN organizing_score >= 20 THEN 'MEDIUM'
       ELSE 'LOW' END AS tier,
  COUNT(*), ROUND(AVG(organizing_score),1) AS avg_score
FROM mergent_employers
WHERE organizing_score IS NOT NULL
GROUP BY 1 ORDER BY 2 DESC;</pre>

<h3>OSHA fuzzy match sample (Tier D)</h3>
<pre>SELECT m.match_confidence, m.match_tier,
  e.estab_name, e.site_city, e.site_state,
  f.employer_name, f.city, f.state
FROM osha_f7_matches m
JOIN osha_establishments e ON m.establishment_id = e.establishment_id
JOIN f7_employers_deduped f ON m.f7_employer_id = f.employer_id
WHERE m.match_tier = 'D' AND m.match_confidence BETWEEN 0.55 AND 0.65
ORDER BY random() LIMIT 50;</pre>

<h3>WHD fuzzy match sample (Tier 6)</h3>
<pre>SELECT m.match_confidence, m.match_tier,
  w.trade_name, w.legal_name, w.city, w.state,
  f.employer_name, f.city AS f_city, f.state AS f_state
FROM whd_f7_matches m
JOIN whd_cases w ON m.case_id = w.case_id
JOIN f7_employers_deduped f ON m.f7_employer_id = f.employer_id
WHERE m.match_tier = '6' AND m.match_confidence BETWEEN 0.55 AND 0.65
ORDER BY random() LIMIT 50;</pre>

<h3>RTW states check</h3>
<pre>SELECT state_code, state_name, enacted_year
FROM ref_rtw_states ORDER BY state_code;</pre>

<h3>Similarity score compression check</h3>
<pre>SELECT
  CASE WHEN similarity_score >= 0.95 THEN '0.95+'
       WHEN similarity_score >= 0.90 THEN '0.90-0.95'
       WHEN similarity_score >= 0.80 THEN '0.80-0.90'
       WHEN similarity_score >= 0.60 THEN '0.60-0.80'
       ELSE '<0.60' END AS bucket,
  COUNT(*), ROUND(100.0*COUNT(*)/SUM(COUNT(*)) OVER(),1) AS pct
FROM mergent_employers
WHERE similarity_score IS NOT NULL
GROUP BY 1 ORDER BY 1 DESC;</pre>

<h3>SAM matching status</h3>
<pre>SELECT match_tier, COUNT(*), ROUND(AVG(match_confidence),3) AS avg_conf
FROM sam_f7_matches GROUP BY 1 ORDER BY 1;</pre>

<h3>Name normalization comparison (SAM vs F7)</h3>
<pre>SELECT s.name_aggressive AS sam_name, f.employer_name_aggressive AS f7_name
FROM sam_entities s
JOIN f7_employers_deduped f ON s.physical_state = f.state
WHERE s.name_aggressive = f.employer_name_aggressive
LIMIT 10;
-- If 0 rows: normalization methods differ</pre>

<p style="margin-top: 40px; border-top: 2px solid #1a1a1a; padding-top: 12px; font-size: 10pt; color: #7f8c8d;">
  <strong>Labor Relations Research Platform - Manual Review Checklist</strong><br>
  Generated February 12, 2026 | 11 sections | ~40 checkbox items<br>
  Print to PDF: Ctrl+P --> Save as PDF
</p>

</body>
</html>