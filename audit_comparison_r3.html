<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Round 3 Audit Comparison — Labor Relations Research Platform</title>
<style>
  @import url('https://fonts.googleapis.com/css2?family=Source+Serif+4:opsz,wght@8..60,400;8..60,600;8..60,700&family=DM+Sans:wght@400;500;600&display=swap');

  :root {
    --bg: #faf9f6;
    --text: #1a1a1a;
    --muted: #5c5c5c;
    --border: #d4d0c8;
    --claude: #d97706;
    --codex: #2563eb;
    --gemini: #059669;
    --claude-bg: #fef3c7;
    --codex-bg: #dbeafe;
    --gemini-bg: #d1fae5;
    --focused-bg: #f3e8ff;
    --focused: #7c3aed;
    --critical: #dc2626;
    --high: #ea580c;
    --medium: #ca8a04;
    --low: #65a30d;
    --positive: #0d9488;
    --sidebar-w: 260px;
    --content-max: 860px;
  }

  * { margin: 0; padding: 0; box-sizing: border-box; }

  body {
    font-family: 'DM Sans', sans-serif;
    background: var(--bg);
    color: var(--text);
    line-height: 1.7;
    font-size: 16px;
  }

  .sidebar {
    position: fixed; top: 0; left: 0;
    width: var(--sidebar-w); height: 100vh;
    overflow-y: auto; background: #2c2825;
    color: #c9c3b8; padding: 24px 16px;
    font-size: 13px; z-index: 100;
    border-right: 3px solid var(--claude);
  }
  .sidebar h2 { color: #fff; font-size: 14px; margin: 18px 0 6px; letter-spacing: 0.5px; text-transform: uppercase; font-family: 'DM Sans', sans-serif; }
  .sidebar a { display: block; color: #b8b0a4; text-decoration: none; padding: 4px 8px; border-radius: 4px; margin: 1px 0; transition: all 0.15s; }
  .sidebar a:hover { background: rgba(255,255,255,0.08); color: #fff; }
  .sidebar .logo { color: var(--claude); font-family: 'Source Serif 4', serif; font-size: 18px; font-weight: 700; margin-bottom: 16px; }

  .container { margin-left: var(--sidebar-w); max-width: var(--content-max); padding: 48px 40px 120px; }

  h1 { font-family: 'Source Serif 4', serif; font-size: 2.2em; font-weight: 700; margin-bottom: 8px; }
  .subtitle { color: var(--muted); font-size: 1.05em; margin-bottom: 36px; }
  h2 { font-family: 'Source Serif 4', serif; font-size: 1.6em; font-weight: 600; margin: 48px 0 12px; padding-top: 24px; border-top: 2px solid var(--border); }
  h3 { font-size: 1.15em; font-weight: 600; margin: 28px 0 8px; }
  h4 { font-size: 1em; font-weight: 600; margin: 20px 0 6px; }

  p { margin: 8px 0 14px; }

  .badge { display: inline-block; padding: 3px 10px; border-radius: 4px; font-size: 12px; font-weight: 600; }
  .badge.claude { background: var(--claude-bg); color: #92400e; }
  .badge.codex { background: var(--codex-bg); color: #1e40af; }
  .badge.gemini { background: var(--gemini-bg); color: #065f46; }
  .badge.focused { background: var(--focused-bg); color: var(--focused); }
  .badge.critical { background: #fef2f2; color: var(--critical); }
  .badge.high { background: #fff7ed; color: var(--high); }
  .badge.medium { background: #fefce8; color: var(--medium); }
  .badge.low { background: #f7fee7; color: var(--low); }
  .badge.positive { background: #f0fdfa; color: var(--positive); }

  .finding { background: #fff; border: 1px solid var(--border); border-radius: 8px; padding: 20px 24px; margin: 16px 0; box-shadow: 0 1px 3px rgba(0,0,0,0.04); }
  .finding h4 { margin-top: 0; }

  .callout { border-left: 4px solid var(--claude); background: var(--claude-bg); padding: 14px 18px; border-radius: 0 6px 6px 0; margin: 14px 0; }
  .callout.blue { border-color: var(--codex); background: var(--codex-bg); }
  .callout.green { border-color: var(--positive); background: #f0fdfa; }
  .callout.red { border-color: var(--critical); background: #fef2f2; }

  table { width: 100%; border-collapse: collapse; margin: 12px 0 20px; font-size: 14px; }
  th { background: #f0ede6; text-align: left; padding: 10px 12px; font-weight: 600; border-bottom: 2px solid var(--border); }
  td { padding: 8px 12px; border-bottom: 1px solid #e8e4dc; vertical-align: top; }
  tr:hover td { background: #faf8f4; }

  .effort-table td:nth-child(2), .effort-table td:nth-child(3), .effort-table td:nth-child(4) { text-align: center; }

  .analogy { font-style: italic; color: var(--muted); }

  .nav-toggle {
    display: none; position: fixed; top: 12px; left: 12px;
    z-index: 200; background: #2c2825; color: #fff;
    border: none; padding: 8px 14px; border-radius: 6px;
    cursor: pointer; font-size: 16px;
  }

  @media (max-width: 900px) {
    .sidebar { transform: translateX(-100%); transition: transform 0.3s; }
    .sidebar.open { transform: translateX(0); }
    .container { margin-left: 0; padding: 24px 18px 80px; }
    .nav-toggle { display: block; }
  }

  @media print {
    .sidebar, .nav-toggle { display: none !important; }
    .container { margin-left: 0; max-width: 100%; }
    .finding { break-inside: avoid; box-shadow: none; }
    h2 { break-before: page; }
  }
</style>
</head>
<body>

<button class="nav-toggle" onclick="document.querySelector('.sidebar').classList.toggle('open')">☰ Sections</button>

<nav class="sidebar">
  <div class="logo">Audit Comparison R3</div>
  <a href="#top">Overview</a>
  <h2>Analysis</h2>
  <a href="#s1">1. Where They All Agree</a>
  <a href="#s2">2. Where They Disagree</a>
  <a href="#s3">3. Unique Catches</a>
  <a href="#s4">4. Methodology</a>
  <h2>Action Plan</h2>
  <a href="#s5">5. Unified Priority List</a>
  <a href="#s6">6. Effort Estimates</a>
  <a href="#s7">7. What to Ignore</a>
  <a href="#s8">8. Pre-Flight Checks</a>
  <h2>Big Picture</h2>
  <a href="#s9">9. What's Working Well</a>
  <a href="#s10">10. Patterns & Red Flags</a>
  <a href="#s11">11. Work Plan</a>
  <a href="#s12">12. Glossary</a>
</nav>

<div class="container" id="top">

<h1>Three-Audit Comparison: Round 3</h1>
<div class="subtitle">Labor Relations Research Platform — February 16, 2026<br>
  <span class="badge claude">Claude Code (Opus 4.6)</span>
  <span class="badge codex">Codex</span>
  <span class="badge gemini">Gemini</span>
  <span class="badge focused">6 Focused Reviews</span>
</div>

<div class="finding">
  <h4>What You're Reading</h4>
  <p>Three AI systems each independently audited the platform — none saw each other's work. Claude also produced 6 additional <em>focused deep-dives</em> on specific areas (matching code, frontend, security, dependencies, test coverage, and database integrity). This document compares everything.</p>
  <p>Think of it like getting three home inspectors plus a specialist plumber, electrician, and structural engineer. Where all three inspectors agree, you can be very confident. The specialists add depth the generalists couldn't reach.</p>
</div>

<!-- ================================================ -->
<h2 id="s1">1. Where They All Agree</h2>
<p>These are the issues <strong>every auditor flagged</strong>. When three independent reviewers find the same problem without coordinating, you can be very confident it's real.</p>

<div class="finding">
  <h4>1A. Authentication Is Turned Off <span class="badge critical">CRITICAL</span></h4>
  <p><strong>What's broken:</strong> The system has a login/password system built in, but it's currently switched off. This means anyone who can reach the server can see all employer data, all admin functions, everything — no username or password required.</p>
  <p><strong>Why it matters:</strong> If the platform is ever accessible from outside your computer (even accidentally), all your organizing intelligence is exposed. An employer could see exactly which companies the platform considers "high priority organizing targets."</p>
  <p><strong>What fixing looks like:</strong> Change one setting in the configuration file (<code>DISABLE_AUTH=false</code>) and register your first admin account. Takes about 5 minutes.</p>
  <p><span class="badge claude">Claude</span> Called it CRITICAL. <span class="badge codex">Codex</span> Called it CRITICAL. <span class="badge gemini">Gemini</span> Called it CRITICAL. All three agree on severity and method.</p>
</div>

<div class="finding">
  <h4>1B. Database Password Exposed in Files <span class="badge critical">CRITICAL</span></h4>
  <p><strong>What's broken:</strong> The actual database password (<code>Juniordog33!</code>) is written out in plain text in at least 14 files — audit scripts, documentation, and helper code. Anyone who can see these files knows the password.</p>
  <p><strong>Why it matters:</strong> With this password, someone could directly connect to the database and read, change, or delete any data. Combined with auth being off, this is the most serious security gap.</p>
  <p><strong>What fixing looks like:</strong> Delete the 6 audit scripts that have the password hardcoded. Remove the password from any documentation files. Then change the password to a new one. The Credential Scan (focused review) found the exact file list.</p>
  <p><span class="badge claude">Claude</span> CRITICAL. <span class="badge codex">Codex</span> HIGH. <span class="badge focused">Credential Scan</span> mapped every occurrence. Minor disagreement on severity — Claude rates it higher because the password is the literal key to the entire database.</p>
</div>

<div class="finding">
  <h4>1C. Documentation Is Significantly Wrong <span class="badge high">HIGH</span></h4>
  <p><strong>What's broken:</strong> The three main documentation files (README, Roadmap, CLAUDE.md) contain about 40 factual errors — wrong employer counts, wrong test counts, wrong scoring formulas, wrong table counts. The CLAUDE.md file describes an entirely different scoring system than what actually exists.</p>
  <p><strong>Why it matters:</strong> Anyone trying to understand or work on the platform (including AI auditors like these three) gets confused by outdated numbers. If an organizer reads that there are 113,713 employers but there are actually 146,863, they're making decisions with wrong baseline information.</p>
  <p><strong>What fixing looks like:</strong> Go through each document and update the numbers. Claude identified 40 specific corrections needed with the old and new values.</p>
  <p><span class="badge claude">Claude</span> HIGH (40 specific errors). <span class="badge codex">Codex</span> HIGH (found the same major discrepancies). <span class="badge gemini">Gemini</span> Didn't check docs (their report only covered Sections 2-5).</p>
</div>

<div class="finding">
  <h4>1D. Dynamic SQL Patterns Are Fragile <span class="badge high">HIGH</span></h4>
  <p><strong>What's broken:</strong> When the system asks the database for data, it sometimes builds those requests by stitching together text strings (called "f-strings"). Right now, the text being stitched in comes from trusted sources inside the code. But if anyone ever changes the code so that user-typed text gets stitched in, an attacker could inject harmful commands.</p>
  <p class="analogy">Analogy: Imagine a form letter where you fill in blanks. Right now the blanks are filled from a pre-approved list. But the system isn't set up to <em>enforce</em> that rule — if a future developer fills a blank from user input, the letter becomes dangerous.</p>
  <p><strong>What fixing looks like:</strong> Replace the string-stitching approach with a safer method that treats table and column names as special protected values. The API Security Fixes (focused review) wrote out exact code replacements for every risky pattern.</p>
  <p><span class="badge claude">Claude</span> MEDIUM (2 specific ORDER BY risks). <span class="badge codex">Codex</span> HIGH (widespread pattern). <span class="badge gemini">Gemini</span> LOW (noted it but said current risk is low). <span class="badge focused">API Security</span> Documented 4 categories with safer replacements. <span class="badge focused">Matching Review</span> Found the same pattern in the matching code.</p>
</div>

<div class="finding">
  <h4>1E. Unused Indexes Wasting Space <span class="badge medium">MEDIUM</span></h4>
  <p><strong>What's broken:</strong> The database has 559 "indexes" — think of these as alphabetical shortcuts that help the database find data faster, like the index in the back of a book. But more than half (300+) of these shortcuts have never been used. They're wasting about 1.5 GB of disk space and slightly slowing down every time data is added or changed.</p>
  <p><strong>What fixing looks like:</strong> Identify which unused indexes are genuinely unnecessary and remove them. Some might be used during rare batch operations, so they need to be checked individually first.</p>
  <p><span class="badge claude">Claude</span> <span class="badge codex">Codex</span> <span class="badge gemini">Gemini</span> All three flagged this. Gemini rated it HIGH; Claude and Codex rated it MEDIUM. The difference: Gemini may have overweighted this because unused indexes are easy to detect and report.</p>
</div>

<div class="finding">
  <h4>1F. Tables Missing Primary Keys <span class="badge medium">MEDIUM</span></h4>
  <p><strong>What's broken:</strong> 14-15 tables (the exact count differs slightly between auditors) don't have a "primary key" — a unique identifier that prevents duplicate rows. Without one, if the same data gets loaded twice by accident, the database can't catch it. This includes important tables like <code>lm_data</code> (union financial reports, 331K rows) and <code>ar_disbursements_emp_off</code> (union spending, 2.8M rows).</p>
  <p class="analogy">Analogy: A primary key is like a Social Security Number for each row — it guarantees every row is unique. Without one, you could accidentally have two identical copies of the same financial report and not know it.</p>
  <p><span class="badge claude">Claude</span> MEDIUM (14 tables). <span class="badge codex">Codex</span> HIGH (15 tables). Slight count difference (likely one borderline table). Codex rates it higher because of the risk of silent data corruption.</p>
</div>

<div class="finding">
  <h4>1G. Materialized Views Are Healthy <span class="badge positive">POSITIVE</span></h4>
  <p>All three auditors confirmed that the 4 materialized views (pre-computed data caches that speed up the system) are populated, the right size, and working correctly. All 186 regular views passed health checks. This is good news — the core query infrastructure is solid.</p>
</div>

<!-- ================================================ -->
<h2 id="s2">2. Where They Disagree</h2>

<div class="finding">
  <h4>2A. How Serious Is the Dynamic SQL Problem?</h4>
  <p>This is the biggest disagreement. All three found it, but they rated it very differently:</p>
  <table>
    <tr><th>Auditor</th><th>Severity</th><th>Reasoning</th></tr>
    <tr><td><span class="badge gemini">Gemini</span></td><td><span class="badge low">LOW</span></td><td>"Current implementation risk is low as query structure is not being manipulated by user input"</td></tr>
    <tr><td><span class="badge claude">Claude</span></td><td><span class="badge medium">MEDIUM</span></td><td>Found 2 specific ORDER BY risks in museums.py and sectors.py</td></tr>
    <tr><td><span class="badge codex">Codex</span></td><td><span class="badge high">HIGH</span></td><td>"Fragile. A small future change can turn a safe fragment into injectable SQL"</td></tr>
  </table>
  <p><strong>My take:</strong> Codex is right. The disagreement is about <em>what to do</em> (method), not <em>whether it's a problem</em>. Gemini looked at the code as it exists today and said "it's fine for now." Codex looked at how easily it could become dangerous tomorrow. Since you're actively developing this platform and will be changing code regularly, the fragile pattern is a ticking clock. The API Security focused review confirmed this and provided exact replacement code.</p>
</div>

<div class="finding">
  <h4>2B. Orphan Counts Don't Match</h4>
  <p>The auditors found different numbers of "orphaned" records (data pointing to things that no longer exist):</p>
  <table>
    <tr><th>Orphan Type</th><th>Claude</th><th>Codex</th><th>Gemini</th></tr>
    <tr><td>Union-employer relation orphans</td><td>824 records</td><td>Not checked specifically</td><td>195 <em>distinct union numbers</em></td></tr>
    <tr><td>Crosswalk orphans</td><td>2,400</td><td>Not checked specifically</td><td>Not found</td></tr>
    <tr><td>NLRB employer xref orphan</td><td>Not checked</td><td>1</td><td>Not checked</td></tr>
  </table>
  <p><strong>My take:</strong> These aren't really contradictions — they measured different things. Gemini counted 195 <em>distinct union file numbers</em> that are orphaned, while Claude counted the total number of <em>records</em> affected (824). It's like saying "3 zip codes have no post office" vs "1,200 people live in zip codes with no post office" — both true, different unit of measurement. The Orphan Map focused review gives the most complete picture: 3,837 total orphan records across 6 relationships.</p>
</div>

<div class="finding">
  <h4>2C. How Many Tables Exist?</h4>
  <table>
    <tr><th>Auditor</th><th>Table Count</th></tr>
    <tr><td><span class="badge claude">Claude</span></td><td>178</td></tr>
    <tr><td><span class="badge codex">Codex</span></td><td>174</td></tr>
    <tr><td><span class="badge gemini">Gemini</span></td><td>Not reported</td></tr>
  </table>
  <p><strong>My take:</strong> The 4-table difference likely comes from how they handled temporary audit tables or tables that were created/dropped during the audit process. It's a minor discrepancy — the important point is both found significantly more tables than the documentation claims (160-169).</p>
</div>

<div class="finding">
  <h4>2D. NLRB case_year Column — Real Problem or Expected?</h4>
  <p><span class="badge gemini">Gemini</span> rated <code>nlrb_cases.case_year</code> being 100% NULL as <strong>CRITICAL</strong> — the strongest finding in their entire report. Neither Claude nor Codex flagged this at all.</p>
  <p><strong>My take:</strong> This needs investigation. If the platform uses <code>earliest_date</code> and <code>latest_date</code> (which are 100% filled) for time-based analysis instead of <code>case_year</code>, then the empty column is a leftover — annoying but not dangerous. But if any code or queries depend on <code>case_year</code>, Gemini is right that it's a serious gap. Check whether any API endpoints or views reference this column.</p>
</div>

<!-- ================================================ -->
<h2 id="s3">3. Unique Catches</h2>
<p>Things only one auditor (or focused review) noticed.</p>

<h3>Only Claude Found</h3>

<div class="finding">
  <h4>3A. OSHA Match False Positive Rate: 53% <span class="badge critical">CRITICAL</span></h4>
  <p>The Match Quality Sample (focused review) randomly checked 15 OSHA matches and found that <strong>8 out of 15 were wrong</strong> — connecting completely different companies. The worst offender is a method called STREET_NUM_ZIP that matches employers just because they share a street number and zip code. A construction company was matched to a botanical garden because they're at the same address.</p>
  <p><strong>Why this matters enormously:</strong> When an organizer looks up an employer and sees OSHA safety violations, those violations might belong to a completely different company. The platform currently reports a 47% OSHA match rate, but the true rate (after removing false positives) is probably closer to 15-20%. This directly undermines the platform's core mission of giving organizers accurate intelligence.</p>
  <p><strong>Why others missed it:</strong> Codex and Gemini checked whether the match <em>tables</em> had proper links (they do — no orphans), but didn't check whether the <em>matches themselves</em> were actually correct. That requires manually looking at matched names side by side, which only the focused review did.</p>
  <p><strong>Assessment: Genuine critical insight.</strong> This is arguably the most important finding across all audits.</p>
</div>

<div class="finding">
  <h4>3B. Scoring Tier Skew — 66% in TOP <span class="badge medium">MEDIUM</span></h4>
  <p>Claude found that 65.9% of all scored employers land in the "TOP" tier. When two-thirds of everything is "top priority," nothing is really prioritized. Codex noted the scoring system exists and works, but didn't evaluate whether the tiers are useful.</p>
  <p><strong>Assessment: Genuine insight.</strong> The tier thresholds need recalibration after temporal decay shifted the score distribution.</p>
</div>

<div class="finding">
  <h4>3C. company_unions Score Factor Always Zero <span class="badge medium">MEDIUM</span></h4>
  <p>One of the 9 scoring factors (<code>score_company_unions</code>) always returns 0 for every employer. It's essentially dead weight. This happens because the scorecard only includes employers matched via OSHA, and those records don't carry union affiliation data.</p>
  <p><strong>Assessment: Genuine insight</strong> — either fix the data flow so this factor works, or remove it and recalibrate to 8 factors.</p>
</div>

<h3>Only Codex Found</h3>

<div class="finding">
  <h4>3D. NLRB Election Duplicates: 1,604 Repeated Case Numbers <span class="badge medium">MEDIUM</span></h4>
  <p>Codex found that the NLRB elections table has 1,604 case numbers that appear more than once. If dashboards assume one row per election, counts get inflated.</p>
  <p><strong>Why others missed it:</strong> Claude and Gemini confirmed there are no duplicate <code>case_number</code> values in <code>nlrb_cases</code>, but Codex specifically checked <code>nlrb_elections</code> — a different table. A case can have multiple election rounds (e.g., initial election, runoff), which is legitimate, but it needs to be handled carefully in counts.</p>
  <p><strong>Assessment: Genuine insight</strong> — needs investigation to determine which are legitimate multi-round elections and which are true duplicates.</p>
</div>

<div class="finding">
  <h4>3E. Accessibility Gaps (No ARIA Labels) <span class="badge medium">MEDIUM</span></h4>
  <p>Codex was the only auditor to check for accessibility features (things that help people using screen readers or keyboard navigation). Found zero <code>aria-*</code> attributes in the main HTML file. Neither Claude nor Gemini mentioned accessibility.</p>
  <p><strong>Assessment: Valid catch</strong> — important if the platform will be used by people with disabilities, or if any compliance requirements apply.</p>
</div>

<div class="finding">
  <h4>3F. Materialized View Staleness Not Tracked <span class="badge medium">MEDIUM</span></h4>
  <p>Codex noticed there's no way to know <em>when</em> the materialized views were last refreshed. If a refresh job fails silently, users get stale data without any warning.</p>
  <p><strong>Assessment: Good catch</strong> — easy fix (add a "last refreshed" timestamp table).</p>
</div>

<h3>Only Gemini Found</h3>

<div class="finding">
  <h4>3G. Data Type Mismatch Causing Performance Drag <span class="badge medium">MEDIUM</span></h4>
  <p>Gemini noticed that <code>latest_union_fnum</code> (stored as an integer) has to be converted to text every time it's joined with <code>unions_master.f_num</code> (stored as text). This type-casting happens in multiple API endpoints and slows down queries.</p>
  <p><strong>Assessment: Valid catch</strong> — fixing the column type would speed up queries, but it's a lower priority than data accuracy issues.</p>
</div>

<div class="finding">
  <h4>3H. Union Name Duplicates in unions_master <span class="badge medium">MEDIUM</span></h4>
  <p>Gemini found the same union name, city, and state combination appearing up to 15 times (e.g., "STATE COUNTY AND MUNI EMPLS AFL-CIO" in Albany, NY). This is likely legitimate (multiple local chapters with similar names), but could confuse analyses.</p>
  <p><strong>Assessment: Informational</strong> — this is expected behavior for union locals, not a bug.</p>
</div>

<h3>Only Focused Reviews Found</h3>

<div class="finding">
  <h4>3I. Address Matcher Uses Wrong Column Names <span class="badge high">HIGH</span></h4>
  <p>The Matching Code Review found that the address matching code uses hardcoded field names (<code>id</code>, <code>name</code>, <code>address</code>) instead of the scenario-specific column names. This means some matching scenarios send the wrong data to the matcher, causing missed matches or incorrect ones.</p>
  <p><strong>Assessment: Critical code bug.</strong> This directly affects match quality and could explain some of the false positives found in the Match Quality Sample.</p>
</div>

<div class="finding">
  <h4>3J. City Filter Ignored in Fuzzy Matching <span class="badge high">HIGH</span></h4>
  <p>The fuzzy matcher accepts a "city" parameter but never actually uses it to filter results. This means same-name employers in the same state but different cities can be incorrectly matched together.</p>
  <p><strong>Assessment: Real bug.</strong> Fixing this would reduce false positive matches.</p>
</div>

<div class="finding">
  <h4>3K. Dependencies Not Pinned <span class="badge medium">MEDIUM</span></h4>
  <p>The Dependency Review found that all software dependencies use minimum versions (like "at least version 2.0") instead of exact versions. This means two different installs could get different software versions, making bugs harder to reproduce.</p>
</div>

<div class="finding">
  <h4>3L. Test Suite Tests Wrong Normalizer <span class="badge medium">MEDIUM</span></h4>
  <p>The Test Coverage Review found that the name normalization tests test a <em>different normalizer</em> than the one used in production matching. The tests pass even if the real matching normalizer is broken.</p>
</div>

<!-- ================================================ -->
<h2 id="s4">4. How They Worked (Methodology)</h2>

<table>
  <tr><th>Aspect</th><th><span class="badge claude">Claude</span></th><th><span class="badge codex">Codex</span></th><th><span class="badge gemini">Gemini</span></th></tr>
  <tr>
    <td><strong>Scope</strong></td>
    <td>10 sections + 6 focused deep-dives covering all areas</td>
    <td>10 sections covering all areas</td>
    <td>4 sections only (Sections 2-5). Missing: inventory, scripts, docs, frontend, security, summary</td>
  </tr>
  <tr>
    <td><strong>Ran live database queries?</strong></td>
    <td>Yes — verified row counts, orphans, index usage, match rates, scoring distributions</td>
    <td>Yes — verified counts, orphans, index usage, cross-references</td>
    <td>Yes — verified column completeness, orphans, view health, index usage</td>
  </tr>
  <tr>
    <td><strong>Ran tests?</strong></td>
    <td>Yes — 359/359 tests passed</td>
    <td>Yes — mentioned pytest results</td>
    <td>Not reported</td>
  </tr>
  <tr>
    <td><strong>Checked match quality?</strong></td>
    <td>Yes — randomly sampled 25 matches and manually evaluated accuracy</td>
    <td>No — checked match table integrity but not match correctness</td>
    <td>No — confirmed referential integrity only</td>
  </tr>
  <tr>
    <td><strong>Reviewed code?</strong></td>
    <td>Yes — read API routers, frontend JS, matching pipeline code</td>
    <td>Yes — read API routers, frontend JS, script structure</td>
    <td>Yes — read API routers (less detail)</td>
  </tr>
  <tr>
    <td><strong>Total findings</strong></td>
    <td>~35 findings + 6 focused reports with ~40 additional findings</td>
    <td>~40 findings across 10 sections</td>
    <td>~20 findings across 4 sections</td>
  </tr>
  <tr>
    <td><strong>Strengths</strong></td>
    <td>Deepest analysis. Only one to check match accuracy. Caught scoring and analytical bugs. Produced the most actionable specific numbers.</td>
    <td>Most systematic and consistent. Best on structural risks (FK enforcement, error handling, accessibility). Most practical quick wins.</td>
    <td>Thorough where it looked. Caught the data type mismatch and case_year gap that others missed.</td>
  </tr>
  <tr>
    <td><strong>Weaknesses</strong></td>
    <td>Produced the most volume — harder to navigate without prioritization</td>
    <td>Didn't check match correctness or scoring distribution quality</td>
    <td>Covered only 4 of 10 sections. No security, docs, frontend, or script review. Much less depth overall.</td>
  </tr>
</table>

<div class="callout">
  <strong>Trust assessment:</strong> Claude's findings are the most trustworthy because of the focused deep-dives and manual verification (especially the match quality sampling). Codex is a close second — very systematic and verified all claims with SQL. Gemini's findings are valid where they exist, but the report only covers about 40% of the audit scope, which means there are large blind spots.
</div>

<!-- ================================================ -->
<h2 id="s5">5. Unified Priority List</h2>

<h3><span class="badge critical">CRITICAL</span> — Fix Before Anything Else</h3>
<table>
  <tr><th>#</th><th>Issue</th><th>Flagged By</th></tr>
  <tr><td>C1</td><td>Enable authentication (currently disabled)</td><td>All three</td></tr>
  <tr><td>C2</td><td>Rotate database password and remove from 14+ files</td><td>Claude, Codex, Credential Scan</td></tr>
  <tr><td>C3</td><td>Fix OSHA match false positives (53% error rate in LOW-confidence matches)</td><td>Claude, Match Quality Sample</td></tr>
</table>

<h3><span class="badge high">HIGH</span> — Fix This Week</h3>
<table>
  <tr><th>#</th><th>Issue</th><th>Flagged By</th></tr>
  <tr><td>H1</td><td>Fix address matcher using wrong column names</td><td>Matching Code Review</td></tr>
  <tr><td>H2</td><td>Add city filter to fuzzy matcher</td><td>Matching Code Review</td></tr>
  <tr><td>H3</td><td>Replace dynamic SQL f-strings with safe identifier composition</td><td>All three + API Security</td></tr>
  <tr><td>H4</td><td>Add admin role checks to 6-9 unprotected admin endpoints</td><td>Claude, Codex</td></tr>
  <tr><td>H5</td><td>Update 40 documentation errors (especially scoring formula)</td><td>Claude, Codex</td></tr>
  <tr><td>H6</td><td>Fix frontend scoring model mismatch (8 vs 9 factors, 80 vs 90 scale)</td><td>Claude, Codex, Frontend Review</td></tr>
</table>

<h3><span class="badge medium">MEDIUM</span> — Fix This Month</h3>
<table>
  <tr><th>#</th><th>Issue</th><th>Flagged By</th></tr>
  <tr><td>M1</td><td>Clean 2,400 crosswalk orphans</td><td>Claude, Orphan Map</td></tr>
  <tr><td>M2</td><td>Recalibrate scoring tier thresholds (66% in TOP is useless)</td><td>Claude</td></tr>
  <tr><td>M3</td><td>Add primary keys to 14 tables missing them</td><td>Claude, Codex</td></tr>
  <tr><td>M4</td><td>Investigate NLRB election duplicates (1,604 repeated case numbers)</td><td>Codex</td></tr>
  <tr><td>M5</td><td>Fix rate limiter (trusts spoofable X-Forwarded-For header)</td><td>Claude</td></tr>
  <tr><td>M6</td><td>Resolve 518 union orphans + 824 relation orphans</td><td>Claude, Gemini, Orphan Map</td></tr>
  <tr><td>M7</td><td>Normalize API error responses (some return 200 with error message)</td><td>Codex</td></tr>
  <tr><td>M8</td><td>Add MV refresh timestamp tracking</td><td>Codex</td></tr>
  <tr><td>M9</td><td>Pin dependencies to exact versions / add lockfile</td><td>Dependency Review</td></tr>
  <tr><td>M10</td><td>Fix tests testing wrong normalizer</td><td>Test Coverage Review</td></tr>
  <tr><td>M11</td><td>Investigate nlrb_cases.case_year 100% NULL</td><td>Gemini</td></tr>
  <tr><td>M12</td><td>Fix data type mismatch (latest_union_fnum int vs f_num text)</td><td>Gemini</td></tr>
</table>

<h3><span class="badge low">LOW</span> — Nice to Have</h3>
<table>
  <tr><th>#</th><th>Issue</th><th>Flagged By</th></tr>
  <tr><td>L1</td><td>Drop unused indexes (save ~1.5 GB)</td><td>All three</td></tr>
  <tr><td>L2</td><td>Archive/drop GLEIF raw schema (12 GB)</td><td>Claude</td></tr>
  <tr><td>L3</td><td>VACUUM FULL on mergent_employers (reclaim ~250 MB)</td><td>Claude</td></tr>
  <tr><td>L4</td><td>Replace inline onclick handlers in JS templates (49-67 remaining)</td><td>Claude, Codex, Frontend Review</td></tr>
  <tr><td>L5</td><td>Add accessibility (ARIA) attributes</td><td>Codex</td></tr>
  <tr><td>L6</td><td>Centralize DB connections (55 scripts bypass shared config)</td><td>Claude</td></tr>
  <tr><td>L7</td><td>Clean up dead DB_CONFIG code in 73 scripts</td><td>Claude</td></tr>
  <tr><td>L8</td><td>Drop splink_match_results table (empty, superseded)</td><td>Claude, Codex</td></tr>
  <tr><td>L9</td><td>Add browser security headers (CSP, HSTS, X-Frame-Options)</td><td>Codex</td></tr>
  <tr><td>L10</td><td>Add token revocation / session invalidation</td><td>Codex</td></tr>
</table>

<!-- ================================================ -->
<h2 id="s6">6. Estimated Effort Per Fix</h2>

<table class="effort-table">
  <tr><th>Item</th><th>Difficulty</th><th>Time</th><th>Risk</th><th>Dependencies / Notes</th></tr>
  <tr><td><strong>C1.</strong> Enable auth</td><td>Easy</td><td>5 min</td><td>Low</td><td>None. Change one config value. Register first admin.</td></tr>
  <tr><td><strong>C2.</strong> Rotate password</td><td>Easy</td><td>30 min</td><td>Low</td><td>Delete 6 audit scripts, scrub docs, change password in .env, restart.</td></tr>
  <tr><td><strong>C3.</strong> Fix OSHA false positives</td><td>Hard</td><td>2-3 days</td><td>Medium</td><td>Reject STREET_NUM_ZIP matches or add name similarity post-filter. Will reduce reported match rate from 47% to ~20%, but remaining matches will be accurate. Requires re-running scorecard after. Depends on: H1, H2.</td></tr>
  <tr><td><strong>H1.</strong> Fix address matcher columns</td><td>Medium</td><td>2-4 hours</td><td>Medium</td><td>Change hardcoded keys to scenario-specific columns. Must re-run matching after.</td></tr>
  <tr><td><strong>H2.</strong> Add city filter to fuzzy</td><td>Easy</td><td>1-2 hours</td><td>Low</td><td>Use the city parameter that's already accepted but ignored.</td></tr>
  <tr><td><strong>H3.</strong> Fix dynamic SQL</td><td>Medium</td><td>1-2 days</td><td>Medium</td><td>Replace f-strings with psycopg2.sql.Identifier. API Security review has exact replacements. Test each endpoint after.</td></tr>
  <tr><td><strong>H4.</strong> Admin role checks</td><td>Easy</td><td>30 min</td><td>Low</td><td>Copy existing pattern from refresh_scorecard to other admin endpoints.</td></tr>
  <tr><td><strong>H5.</strong> Update docs</td><td>Easy</td><td>1-2 hours</td><td>Low</td><td>Claude listed all 40 corrections with old→new values.</td></tr>
  <tr><td><strong>H6.</strong> Fix frontend scoring</td><td>Medium</td><td>2-4 hours</td><td>Low</td><td>Decide on 8 or 9 factors, update config.js, scorecard.js, and HTML legend.</td></tr>
  <tr><td><strong>M1.</strong> Clean crosswalk orphans</td><td>Medium</td><td>2-4 hours</td><td>Medium</td><td>Either re-match 2,400 entries to current employer IDs or delete. Verify corporate lookups work after.</td></tr>
  <tr><td><strong>M2.</strong> Recalibrate tiers</td><td>Easy</td><td>1 hour</td><td>Low</td><td>Adjust threshold numbers. Depends on: H6 (scoring model alignment).</td></tr>
  <tr><td><strong>M3.</strong> Add primary keys</td><td>Medium</td><td>2-4 hours</td><td>Medium</td><td>Must verify no duplicates exist first (run dedup queries). If duplicates found, resolve them before adding PK.</td></tr>
  <tr><td><strong>M4.</strong> NLRB election dupes</td><td>Medium</td><td>2-4 hours</td><td>Low</td><td>Investigate which are legitimate (runoff elections) vs true duplicates.</td></tr>
  <tr><td><strong>M5-M12</strong></td><td>Easy-Med</td><td>1-4 hrs each</td><td>Low</td><td>Independent tasks that can be done in any order.</td></tr>
  <tr><td><strong>L1-L10</strong></td><td>Easy</td><td>15 min-2 hrs each</td><td>Low</td><td>Cleanup tasks with minimal risk.</td></tr>
</table>

<div class="callout blue">
  <strong>What makes something "Hard"?</strong> C3 (OSHA false positives) is the hardest because it touches the matching pipeline, requires re-running matches on 1M+ OSHA records, rebuilding the scorecard, and potentially changes the numbers organizers see. Everything downstream of matching changes. "Easy" items are single-file config changes or copy-paste patterns from existing code.
</div>

<!-- ================================================ -->
<h2 id="s7">7. What to Ignore (and Why)</h2>

<div class="finding">
  <h4>Skip: Archive GLEIF Schema (12 GB)</h4>
  <p>Claude suggested archiving the raw GLEIF data to save disk space. But 12 GB is tiny by modern standards, and you might need it for future corporate hierarchy analysis. <strong>Skip until disk space becomes an actual constraint.</strong></p>
</div>

<div class="finding">
  <h4>Skip: Token Revocation System</h4>
  <p>Codex suggested adding a token blacklist so you can force-logout users. With auth currently disabled and only one likely user (you), this is premature. <strong>Revisit when you have multiple users.</strong></p>
</div>

<div class="finding">
  <h4>Skip: Browser Security Headers (CSP, HSTS)</h4>
  <p>Codex recommended adding HTTP security headers. These matter when the platform is deployed on the public internet. If it's running locally, these add complexity with no benefit. <strong>Revisit when deploying publicly.</strong></p>
</div>

<div class="finding">
  <h4>Skip: Full Frontend Accessibility Overhaul</h4>
  <p>Codex found no ARIA attributes. A full accessibility rewrite would be significant work. <strong>Worth doing eventually, but not before the data quality and security issues are resolved.</strong></p>
</div>

<div class="finding">
  <h4>Skip: Gemini's Concern About Union Name Duplicates</h4>
  <p>Multiple locals of the same union (like AFSCME Local 100 and AFSCME Local 200 both in Albany, NY) is expected behavior, not a bug. <strong>No action needed.</strong></p>
</div>

<!-- ================================================ -->
<h2 id="s8">8. Pre-Flight Checks (Questions Before Starting)</h2>

<div class="callout red">
<h4>Before C1 (Enable Auth):</h4>
<p>• Is the <code>LABOR_JWT_SECRET</code> environment variable set to a real, long random string? (Not the example value from .env.example)<br>
• Do you have a plan for creating your admin account? The first user registration should be done immediately after enabling auth, before anyone else could register.</p>
</div>

<div class="callout red">
<h4>Before C2 (Rotate Password):</h4>
<p>• Are there any scheduled scripts or cron jobs that connect to the database using the old password? Changing it will break them until they're updated.<br>
• Is the .env file the <em>only</em> place the new password will live? (Don't put it in any scripts or docs.)</p>
</div>

<div class="callout red">
<h4>Before C3 (Fix OSHA Matches):</h4>
<p>• How do you want to handle the 102K LOW-confidence matches? Options: (a) delete them entirely, (b) add a name-similarity filter and re-evaluate, or (c) quarantine them (keep in DB but hide from user-facing views).<br>
• Are you prepared for the match rate to drop from 47% to ~20%? The remaining matches will be accurate, but the headline number gets smaller.<br>
• Do you want to rebuild the scorecard immediately after, or batch the matching fix with H1/H2 and rebuild once?</p>
</div>

<div class="callout">
<h4>Before H3 (Fix Dynamic SQL):</h4>
<p>• The API Security review wrote out exact replacement code. Do you want an AI to apply these changes and run the test suite afterward, or do them manually?<br>
• After changes, every API endpoint that was modified needs to be tested. The test suite covers some but not all endpoints.</p>
</div>

<div class="callout">
<h4>Before H6 (Frontend Scoring Fix):</h4>
<p>• First decide: is the platform using 8 factors or 9? The backend has 9 columns but one (company_unions) is always zero. Should you remove it and call it an 8-factor model, or fix the data so all 9 work?<br>
• Once decided, update config.js SCORE_MAX, the scorecard renderer, and the HTML legend all at once.</p>
</div>

<!-- ================================================ -->
<h2 id="s9">9. What's Actually Working Well</h2>
<p>All three auditors confirmed these things are solid. <strong>Don't mess with them.</strong></p>

<div class="finding">
  <h4>✅ Deduplication Pipeline <span class="badge positive">EXCELLENT</span></h4>
  <p>The system reduces 70.1M raw reported union members to 14.5M deduplicated — <strong>within 1.5% of the official BLS figure</strong> (14.3M). It correctly handles multi-year inflation, parent-child hierarchy double-counting, and cross-filing. This is the most impressive analytical achievement of the platform and should not be changed.</p>
</div>

<div class="finding">
  <h4>✅ Core Data Completeness <span class="badge positive">EXCELLENT</span></h4>
  <p>The main employer table has 100% fill rates on names and IDs, 97%+ on geographic fields, and 85% on industry codes (up from a documented 38%). The unions table has 100% on names and identifiers. NLRB elections are 99%+ complete on key fields. The raw data quality is very strong.</p>
</div>

<div class="finding">
  <h4>✅ API Architecture <span class="badge positive">GOOD</span></h4>
  <p>The refactoring from one monolithic file to 17 organized router modules is working well. All three auditors noted this as a good design decision. The modals.js monolith was split into 7 feature modules. The API base URL is dynamically configured, not hardcoded.</p>
</div>

<div class="finding">
  <h4>✅ Materialized Views <span class="badge positive">GOOD</span></h4>
  <p>All 4 materialized views are populated, properly sized, and working. All 186 regular views pass health checks. Zero broken views. The query infrastructure is solid.</p>
</div>

<div class="finding">
  <h4>✅ Test Suite <span class="badge positive">GOOD</span></h4>
  <p>359 out of 359 tests pass. Coverage includes matching, scoring, data integrity, propensity modeling, and temporal decay. This is much better than most projects at this stage. (The Test Coverage Review identifies gaps to fill, but the existing tests are valuable.)</p>
</div>

<div class="finding">
  <h4>✅ Matching Pipeline Architecture <span class="badge positive">GOOD</span></h4>
  <p>The 6-tier matching cascade, adapter pattern, and unified match log are well-designed. New data sources can be added easily. The architecture is sound — the problems are in specific implementation details (column names, city filter), not the overall design.</p>
</div>

<div class="finding">
  <h4>✅ Data Freshness Tracking <span class="badge positive">GOOD</span></h4>
  <p>19 data sources tracked with timestamps and record counts. The frontend shows freshness indicators. This kind of operational monitoring is often missing from research platforms.</p>
</div>

<!-- ================================================ -->
<h2 id="s10">10. Patterns and Red Flags</h2>

<h3>Recurring Theme 1: Quality vs Quantity Tension</h3>
<p>The platform has prioritized coverage (connecting as many data sources as possible) over accuracy (making sure each connection is correct). The OSHA match rate is reported as 47%, but the true accuracy is much lower. The scoring system covers 22,389 employers but can't meaningfully differentiate between them (66% in TOP tier). The trend is: impressive breadth, but depth needs tightening.</p>

<h3>Recurring Theme 2: Documentation Lag</h3>
<p>The platform has gone through 5 development phases, and the documentation hasn't kept up. Almost every number in the docs is stale. This isn't surprising for a fast-moving project, but it means the docs are more likely to mislead than help at this point.</p>

<h3>Recurring Theme 3: Security Is Built But Disabled</h3>
<p>The auth system, JWT tokens, role-based access — they all exist and are properly coded. They're just turned off. This suggests the platform was built with deployment in mind but hasn't gone through the "hardening" step of actually enforcing security. The gap between "built" and "enabled" is small but critical.</p>

<h3>Recurring Theme 4: Script Sprawl</h3>
<p>544-548 Python scripts, with critical pipeline scripts mixed in with one-off diagnostic tools. Both Claude and Codex flagged this. It's not an urgent problem, but it increases the risk of running the wrong script or skipping a needed one during rebuilds.</p>

<h3>Areas No Auditor Examined</h3>
<p>The following areas received little or no attention across all audits:</p>
<p>• <strong>Performance under load</strong> — no auditor tested how the API performs with many simultaneous users.<br>
• <strong>Backup and recovery</strong> — no auditor checked whether backups exist or tested a restore.<br>
• <strong>Data update pipelines</strong> — nobody tested what happens when you load a new month of OSHA or NLRB data. Does it merge correctly? Are matches updated?<br>
• <strong>Public sector data quality</strong> — all auditors focused heavily on the F7/OSHA/NLRB core. The 7,987 public sector employers got minimal scrutiny.</p>

<!-- ================================================ -->
<h2 id="s11">11. Suggested Work Plan</h2>

<h3>Phase 1: Security Lockdown (Day 1, ~2 hours)</h3>
<p>These can all be done in one sitting with very low risk:</p>
<p>1. Enable auth (C1) — 5 minutes<br>
2. Register your admin account — 2 minutes<br>
3. Delete the 6 audit scripts with hardcoded password — 5 minutes<br>
4. Remove password from documentation files — 15 minutes<br>
5. Rotate the database password to something new — 10 minutes<br>
6. Add admin role checks to the 9 unprotected endpoints (H4) — 30 minutes</p>
<div class="callout green"><strong>Checkpoint:</strong> Verify you can log in, that unauthenticated requests are rejected, and that admin endpoints require admin role.</div>

<h3>Phase 2: Matching Accuracy (Days 2-4, ~2-3 days)</h3>
<p>These fixes are connected and should be done together, then the scorecard rebuilt once:</p>
<p>1. Fix address matcher column names (H1) — 2-4 hours<br>
2. Add city filter to fuzzy matcher (H2) — 1-2 hours<br>
3. Add name-similarity post-filter to OSHA matches (C3) — 4-8 hours<br>
4. Re-run OSHA matching with the fixed pipeline<br>
5. Rebuild the scorecard materialized view</p>
<div class="callout green"><strong>Checkpoint:</strong> Sample 20 random OSHA matches from the new results. Are at least 80% clearly correct? Check new match rate — should be 15-25% but accurate. Verify scorecard still has reasonable score distribution.</div>

<h3>Phase 3: API & Frontend Alignment (Days 5-7, ~2-3 days)</h3>
<p>These can be done in parallel if desired:</p>
<p>1. Fix dynamic SQL patterns (H3) — 1-2 days (can run alongside #2)<br>
2. Align frontend scoring model (H6) — 2-4 hours<br>
3. Recalibrate scoring tier thresholds (M2) — 1 hour<br>
4. Fix API error response consistency (M7) — 2-4 hours</p>
<div class="callout green"><strong>Checkpoint:</strong> Run full test suite (should still be 359/359 passing). Open the frontend and check scorecard display — do factors, scales, and tiers make sense?</div>

<h3>Phase 4: Data Cleanup (Week 2, ~3-5 days)</h3>
<p>1. Clean 2,400 crosswalk orphans (M1)<br>
2. Resolve 518+824 union orphans (M6)<br>
3. Add primary keys to 14 tables (M3, after dedup verification)<br>
4. Investigate NLRB election duplicates (M4)<br>
5. Update all documentation (H5)<br>
6. Investigate case_year column (M11)</p>
<div class="callout green"><strong>Checkpoint:</strong> Run orphan checks — should be near zero. Spot-check corporate lookups that were affected by crosswalk orphans.</div>

<h3>Phase 5: Infrastructure Hardening (Week 3, as time allows)</h3>
<p>These are independent cleanup tasks in any order:<br>
Pin dependencies (M9), fix test normalizer (M10), add MV refresh tracking (M8), drop unused indexes (L1), clean up script sprawl (L6, L7), fix rate limiter (M5)</p>

<!-- ================================================ -->
<h2 id="s12">12. Glossary</h2>

<table>
  <tr><th>Term</th><th>What It Means</th><th>Why It Matters Here</th></tr>
  <tr><td><strong>Primary Key</strong></td><td>A column (or set of columns) that uniquely identifies each row in a table — like a Social Security Number for data. The database prevents two rows from having the same primary key.</td><td>14 tables are missing one, which means duplicate rows can sneak in without detection — dangerous for financial data like union spending reports.</td></tr>
  <tr><td><strong>Foreign Key</strong></td><td>A rule that says "this column must point to a valid row in another table." If you try to insert a reference to something that doesn't exist, the database stops you.</td><td>Most of the platform's cross-references between tables don't have foreign keys enforced, which is how orphaned records (bookmarks to deleted pages) accumulate.</td></tr>
  <tr><td><strong>Orphan / Orphaned Record</strong></td><td>A record that points to another record that no longer exists — like a bookmark to a deleted webpage. When you click it, you get nothing.</td><td>The platform has 3,837 orphans across 6 relationships. The biggest cluster (2,400) affects corporate identity lookups.</td></tr>
  <tr><td><strong>Index</strong></td><td>A shortcut structure that helps the database find data faster, like the index in the back of a book. Without one, the database has to read every row to find what it's looking for.</td><td>The platform has 559 indexes, but 300+ are never used — they waste 1.5 GB of space and slow down data writes.</td></tr>
  <tr><td><strong>Materialized View (MV)</strong></td><td>A pre-computed query result stored as a table. Instead of running a complex calculation every time someone asks, the answer is cached. It needs to be "refreshed" when the underlying data changes.</td><td>The 4 MVs are the backbone of the platform's speed — the scorecard, employer search, and wage violation summaries are all materialized views.</td></tr>
  <tr><td><strong>SQL Injection</strong></td><td>An attack where someone tricks the database into running harmful commands by inserting special characters into input fields. Like typing "delete everything" into a search box and having it actually work.</td><td>The platform builds some database queries by stitching text together (f-strings). If user-typed text ever gets into those stitched queries, an attacker could read or destroy data.</td></tr>
  <tr><td><strong>F-string</strong></td><td>A Python shortcut for building text by inserting variables. Like a fill-in-the-blank template: "Hello {name}" becomes "Hello Jacob." The problem is when the blank gets filled with something dangerous.</td><td>The API uses f-strings to build database queries. Currently the blanks are filled from safe sources, but the pattern is fragile if code changes later.</td></tr>
  <tr><td><strong>JWT (JSON Web Token)</strong></td><td>A digital "hall pass" the server gives you after you log in. You show it with every request to prove your identity. It expires after a set time (8 hours in this platform).</td><td>The JWT system is built correctly but disabled. When enabled, it controls who can access what.</td></tr>
  <tr><td><strong>CORS</strong></td><td>A browser security rule that controls which websites can talk to your server. Like a bouncer checking IDs — only approved websites get in.</td><td>Properly configured in this platform (only localhost allowed). This is good.</td></tr>
  <tr><td><strong>Fuzzy Matching</strong></td><td>Connecting records that are similar but not identical — "ACME Corp" and "Acme Corporation" are the same company but spelled differently. The system uses various methods (name similarity, address matching, industry codes) to find these connections.</td><td>Fuzzy matching is 210x slower than exact matching and produces most of the false positives. The STREET_NUM_ZIP method (matching by address only) is the biggest source of errors.</td></tr>
  <tr><td><strong>Trigram</strong></td><td>A technique for measuring how similar two text strings are by breaking them into groups of 3 characters. "cat" becomes {" c", "ca", "at", "t "}. Two strings with many shared trigrams are likely similar.</td><td>Used in fuzzy name matching. The trigram indexes (53-57 MB each) support this, but most show zero usage since the last stats reset.</td></tr>
  <tr><td><strong>NAICS Code</strong></td><td>A standardized industry classification code. Like a zip code for industries — "238" means "Specialty Trade Contractors." Employers get assigned one based on what they do.</td><td>85% of employers now have NAICS codes (up from a documented 38%). The codes are used for industry-based scoring and finding comparable employers.</td></tr>
  <tr><td><strong>Temporal Decay</strong></td><td>Reducing the weight of older data so recent information counts more. A safety violation from last year is more relevant than one from 10 years ago.</td><td>The scoring system applies temporal decay to OSHA violations, which shifted the entire score distribution and is why the tier thresholds need recalibration.</td></tr>
  <tr><td><strong>API Endpoint</strong></td><td>A specific URL that the system responds to with data. Like a phone extension — you call the main number (the API) and dial an extension (/api/employers/search) to reach a specific service.</td><td>The platform has 160-161 endpoints across 17 topic groups. 6-9 admin endpoints lack proper access controls.</td></tr>
  <tr><td><strong>Rate Limiting</strong></td><td>Restricting how many requests someone can make in a given time period — like limiting phone calls to prevent harassment. The platform's login rate limit (10 attempts per 5 minutes) can be bypassed by spoofing the X-Forwarded-For header.</td><td>Without effective rate limiting, an attacker could try unlimited passwords.</td></tr>
  <tr><td><strong>Lockfile</strong></td><td>A file that records the exact version of every software library installed. Without one, two different computers could install different versions, causing "works on my machine" bugs.</td><td>The platform doesn't have a lockfile, making it harder to reproduce the exact same setup on a different computer.</td></tr>
  <tr><td><strong>VACUUM FULL</strong></td><td>A database maintenance command that reclaims disk space from deleted rows. When you delete data, the space isn't immediately freed — it's like removing books from a shelf but leaving the shelf. VACUUM FULL removes the empty shelves.</td><td>The mergent_employers table has ~250 MB of wasted space from deleted rows.</td></tr>
</table>

</div><!-- container -->

<script>
  // Smooth scroll for sidebar links
  document.querySelectorAll('.sidebar a').forEach(a => {
    a.addEventListener('click', e => {
      if (a.hash) {
        e.preventDefault();
        document.querySelector(a.hash)?.scrollIntoView({ behavior: 'smooth', block: 'start' });
        document.querySelector('.sidebar').classList.remove('open');
      }
    });
  });
</script>
</body>
</html>
